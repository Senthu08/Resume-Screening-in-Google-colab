 !mkdir ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
# !kaggle datasets download -d gauravduttakiit/resume-dataset
# !mkdir data
# !unzip resume-dataset.zip -d /content/data

# # !rm -r  ./data

import numpy as np
import pandas as pd
import tensorflow as tf

import re
import string
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping

from warnings import filterwarnings
filterwarnings('ignore')

data = pd.read_csv('./UpdatedResumeDataSet.csv')

data.head()

data['Category'].value_counts()

x_axis = data['Category'].value_counts().index.to_list()

y_axis = data['Category'].value_counts().to_list()

def barPlot():
  fig, ax = plt.subplots(figsize =(16, 9))

  # Horizontal Bar Plot
  ax.barh(x_axis,y_axis)

  for s in ['top', 'bottom', 'left', 'right']:
      ax.spines[s].set_visible(False)
  
  # Remove x, y Ticks
  ax.xaxis.set_ticks_position('none')
  ax.yaxis.set_ticks_position('none')
  
  # Add padding between axes and labels
  ax.xaxis.set_tick_params(pad = 5)
  ax.yaxis.set_tick_params(pad = 10)
  
  # Add x, y gridlines
  ax.grid(b = True, color ='grey',
          linestyle ='-.', linewidth = 0.5,
          alpha = 0.2)
  
  # Show top values
  ax.invert_yaxis()
  
  # Add annotation to bars
  for i in ax.patches:
      plt.text(i.get_width()+0.2, i.get_y()+0.5,
              str(round((i.get_width()), 2)),
              fontsize = 10, fontweight ='bold',
              color ='grey')
  
  # Add Plot Title
  ax.set_title('Category',loc ='left', )
  
  # Add Text watermark
  fig.text(0.9, 0.15, 'Jeeteshgavande30', fontsize = 12,color ='grey', ha ='right', va ='bottom',alpha = 0.7)
  
  # Show Plot
  plt.show()

barPlot()

def custom_standardization(Text):
    Text = re.sub('<br />', ' ', Text) #removing HTML
    Text = re.sub('https?://\S+|www\.\S+','', Text)#removing hyperlink
    Text = re.sub('RT|cc','', Text)# remove RT and cc
    Text = re.sub('#\S+','', Text) # remove hashtags

    Text = re.sub('\[.*?\]','', Text) #removing square brackets  
    Text = re.sub('[^\x00-\x7f]','', Text) 

    Text = re.sub( '[%s]' % re.escape(string.punctuation),'', Text) #removing puncuation
    Text = re.sub('\w*\d\w*','', Text)#remove words containing numbers
    return Text
    
    data['CleanResume'] = data.Resume.apply(lambda x: custom_standardization(x))
data.head()

X = data['CleanResume']
y = data['Category']

# Replacing space between charecter(e.g "Data Science" to "DataScience") 
# To get single array EX=> {Data:6},{Science:7},{HR:8}
# So we will have some array of 2D and some of 1D. That can cause the problem..
for i in range(len(y)):
  y[i] = y[i].replace(" ","")


X_train, X_test, y_train,y_test = train_test_split(X,y, random_state=42, shuffle=True, test_size=0.17)

vocab_size = 6000
embedding_dim = 16
max_length = 6000
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, lower=True)
tokenizer.fit_on_texts(X)
word_index = tokenizer.word_index

X_sequences = tokenizer.texts_to_sequences(X_train)
X_padded = pad_sequences(X_sequences,maxlen=max_length,padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(X_test)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)

tokenizer = Tokenizer(lower=True)
tokenizer.fit_on_texts(y)
word_index = tokenizer.word_index

y_sequences = tokenizer.texts_to_sequences(y_train)
# y_padded = pad_sequences(y_sequences,maxlen=300,padding=padding_type, truncating=trunc_type)

y_testing_sequences = tokenizer.texts_to_sequences(y_test)
# y_testing_padded = pad_sequences(y_testing_sequences,maxlen=300, padding=padding_type, truncating=trunc_type)

y_sequences

embedding_dim = 64
# input vocab of size 6000, and output embedding dimension of size 64 

model = Sequential([
  Embedding(vocab_size, embedding_dim,),
  Bidirectional(LSTM(64)),
  Dense(64, activation='relu'),
  Dense(26, activation='softmax')# 26 lables are there....
])

model.summary()
# loss = tf.keras.losses.sparse_categorical_crossentropy(from_logits=False)
# loss = tf.keras.losses.categorical_crossentropy(from_logits=True)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

y_padded = np.array(y_sequences)
y_testing_padded = np.array(y_testing_sequences)

earlyStopping = EarlyStopping(monitor='val_accuracy',patience=1)

history = model.fit(X_padded,y_padded,epochs = 10, validation_data=(testing_padded,y_testing_padded ), callbacks=[earlyStopping])
score = model.evaluate(testing_padded, y_testing_padded, verbose=1)
print("Test Score:", score[0])
print("Test Accuracy:", score[1])
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()
